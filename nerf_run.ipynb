{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "# %env CUDA_VISIBLE_DEVICES=0\n",
    "# %env XLA_PYTHON_CLIENT_PREALLOCATE=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from random import sample as sample_no_duplicates\n",
    "\n",
    "import cv2\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from flax import linen as nn\n",
    "from flax import optim, serialization\n",
    "from jax import grad, jit, random, value_and_grad, vmap\n",
    "from jax.config import config\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from tqdm.notebook import tqdm\n",
    "config.enable_omnistaging()\n",
    "\n",
    "# Generate key which is used to generate random numbers\n",
    "rng = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class DatasetEnum(Enum):\n",
    "    NERF = \"nerf_example\"\n",
    "    TINY = \"tiny_nerf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "cfg={\n",
    "    \"gpu_id\": 0,\n",
    "    \"lr\": 0.001,\n",
    "    \"num_epochs\": 500,\n",
    "    \"comments\": \"Final model + use 3D for view dir\",\n",
    "    \"near_clip\": 2.0,\n",
    "    \"far_clip\": 6.0,\n",
    "    \"ray_sample\": 32,\n",
    "    \"fine_ray_sample\": 64,\n",
    "    \"encode_num\": 10,\n",
    "    \"encode_num_view\": 4,\n",
    "    \"layer_num\": 6,\n",
    "    \"hidden_ch\": 128,\n",
    "    \"dataset_type\": \"NERF\",  # \"NERF\", \"TINY\"\n",
    "    \"half_res\": True,\n",
    "    \"testset_num\": 32,\n",
    "    \"workers\": 4,\n",
    "}\n",
    "cfg = AttrDict(cfg)\n",
    "\n",
    "\n",
    "# import wandb\n",
    "# wandb.init(\n",
    "#     project=\"nerf_practice\",\n",
    "#     config=cfg,\n",
    "# )\n",
    "# cfg = wandb.config\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_dim = 3\n",
    "view_dim = 3\n",
    "if not os.path.exists('./results'):\n",
    "    os.mkdir('./results')\n",
    "if not os.path.exists('./checkpoints'):\n",
    "    os.mkdir('./checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = getattr(DatasetEnum, cfg.dataset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class TinyNeRFDataset(Dataset):\n",
    "    def __init__(self, npy_filename, split_type=\"train\", transform=None):\n",
    "        # Load data\n",
    "        data = np.load(npy_filename)\n",
    "        self.samples = []\n",
    "        for img, pose in zip(data[\"images\"], data[\"poses\"]):\n",
    "            # Adjust poses so that camera front is z+\n",
    "            T = np.eye(3)\n",
    "            T[1, 1] = -1\n",
    "            T[2, 2] = -1\n",
    "            pose[:3, :3] = pose[:3, :3].dot(T)\n",
    "            self.samples.append((img, pose))\n",
    "        self.focal = data[\"focal\"]\n",
    "\n",
    "        test_idx = 100\n",
    "        if split_type == \"train\":\n",
    "            self.samples = self.samples[:test_idx]\n",
    "        else:\n",
    "            self.samples = self.samples[test_idx:]\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, pose = self.samples[idx]\n",
    "        sample = {\"img\": img, \"pose\": pose}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        sample[\"focal\"] = self.focal\n",
    "        return sample\n",
    "\n",
    "\n",
    "class NeRFDataset(Dataset):\n",
    "    object_names = [\n",
    "        \"chair\",\n",
    "        \"drums\",\n",
    "        \"ficus\",\n",
    "        \"hotdog\",\n",
    "        \"lego\",\n",
    "        \"materials\",\n",
    "        \"mic\",\n",
    "        \"ship\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root_dir, split_type=\"train\", transform=None, half_res=False):\n",
    "        # Load data\n",
    "        self.root_dir = root_dir\n",
    "        self.half_res = half_res\n",
    "        json_filename = join(self.root_dir, f\"transforms_{split_type}.json\")\n",
    "        with open(json_filename) as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        self.samples = []\n",
    "        for frame in json_data[\"frames\"]:\n",
    "            img_fname = join(self.root_dir, frame[\"file_path\"] + \".png\")\n",
    "\n",
    "            pose = np.array(frame[\"transform_matrix\"])\n",
    "            # Adjust poses so that camera front is z+\n",
    "            T = np.eye(3)\n",
    "            T[1, 1] = -1\n",
    "            T[2, 2] = -1\n",
    "            pose[:3, :3] = pose[:3, :3].dot(T)\n",
    "            self.samples.append((img_fname, pose))\n",
    "\n",
    "        # focal length\n",
    "        img = self.load_image(img_fname)\n",
    "        h, w = img.shape[:2]\n",
    "        camera_angle_x = float(json_data[\"camera_angle_x\"])\n",
    "        self.focal = 0.5 * w / np.tan(0.5 * camera_angle_x)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_image(self, img_fname):\n",
    "        img = cv2.imread(img_fname, cv2.IMREAD_UNCHANGED)\n",
    "        # Downsample\n",
    "        if self.half_res:\n",
    "            img = cv2.resize(img, (0, 0), fx=0.5, fy=0.5)\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "\n",
    "        if img.shape[2] == 4:\n",
    "            # Add arbitrary background color for image with alpha\n",
    "            bkg = np.array([1.0, 1.0, 1.0], dtype=np.float32)\n",
    "            img_rgb = img[..., :3]\n",
    "            img_alpha = img[..., 3:]\n",
    "            img = img_rgb*img_alpha + bkg*(1.-img_alpha) \n",
    "        # BGR to RGB\n",
    "        img = np.array(img[...,::-1])\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_fname, pose = self.samples[idx]\n",
    "        img = self.load_image(img_fname)\n",
    "        sample = {\"img\": img, \"pose\": pose}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        sample[\"focal\"] = self.focal\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/'):\n",
    "    # Load dataset using official script\n",
    "    !wget https://raw.githubusercontent.com/bmild/nerf/master/download_example_data.sh\n",
    "    !bash download_example_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "print(\"Dataset type:\", DATASET)\n",
    "if DATASET == DatasetEnum.NERF:\n",
    "    root_dir = './data/nerf_synthetic/lego'\n",
    "    if not os.path.split(root_dir)[-1] in NeRFDataset.object_names:\n",
    "        print(\"The object might not be included\")\n",
    "    trainset = NeRFDataset(root_dir, split_type=\"train\", half_res=cfg.half_res)\n",
    "    all_testset = NeRFDataset(root_dir, split_type=\"test\", half_res=cfg.half_res)\n",
    "elif DATASET == DatasetEnum.TINY:\n",
    "    tiny_data = \"tiny_nerf_data.npz\"\n",
    "    trainset = TinyNeRFDataset(tiny_data, split_type=\"train\")\n",
    "    all_testset = TinyNeRFDataset(tiny_data, split_type=\"test\")\n",
    "    cfg.testset_num = 0\n",
    "else:\n",
    "    raise Exception(\"Unknown dataset\")\n",
    "\n",
    "if cfg.testset_num > 0:\n",
    "    testset = Subset(all_testset, range(cfg.testset_num))\n",
    "else:\n",
    "    testset = all_testset\n",
    "print(f\"Dataset size:\", len(trainset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawAxis(T=np.eye(4), scale=0.3, colors=[\"r\", \"g\", \"b\"]):\n",
    "    tvec = T[:3, 3]\n",
    "    R = T[:3, :3]\n",
    "    start = tvec.flatten()[np.newaxis].repeat(3, axis=0)\n",
    "    end = start + scale * R.T\n",
    "    for s, e, c in zip(start, end, colors):\n",
    "        ax.plot([s[0], e[0]], [s[1], e[1]], [s[2], e[2]], c=c)\n",
    "\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "sample = trainset[0]\n",
    "img = sample['img']\n",
    "pose = sample['pose']\n",
    "focal = sample['focal']\n",
    "print(\"focal:\", focal)\n",
    "\n",
    "camera_bnd = np.array([it['pose'][:3, 3] for it in trainset])\n",
    "print(\"Camera position range\")\n",
    "print(camera_bnd.max())\n",
    "print(camera_bnd.min())\n",
    "camera_bnd_max = max(abs(camera_bnd.max()), abs(camera_bnd.min()))\n",
    "print(camera_bnd_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sample = {key: [] for key in batch[0].keys()}\n",
    "    for it in batch:\n",
    "        for key, val in it.items():\n",
    "            sample[key].append(val)\n",
    "    sample = {key: np.stack(val) for key, val in sample.items()}\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from batch\n",
    "batch = iter(DataLoader(trainset, batch_size=32, shuffle=False, collate_fn=collate_fn)).next()\n",
    "imgs = batch[\"img\"]\n",
    "poses = batch[\"pose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "fig, ax = plt.subplots(1, 4, figsize=(7, 4))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(imgs[idx + i])\n",
    "\n",
    "# 3D Plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "# ax.axis('off')\n",
    "c = [\"c\", \"y\", \"k\", \"b\"]\n",
    "\n",
    "for i in range(4):\n",
    "    colors = [\"r\", \"g\", c[i]]\n",
    "    drawAxis(poses[idx + i], 0.6, colors)\n",
    "\n",
    "# Origin\n",
    "drawAxis()\n",
    "\n",
    "# adjust scale\n",
    "scale = 7\n",
    "start = np.zeros((3, 3)) - scale / 2\n",
    "end = start + scale * np.eye(3)\n",
    "for s, e in zip(start, end):\n",
    "    ax.plot([s[0], e[0]], [s[1], e[1]], [s[2], e[2]], c=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera pose generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spherical_to_xyz(r, theta, phi):\n",
    "    assert theta.shape == phi.shape == r.shape\n",
    "    x = r * jnp.sin(theta) * jnp.cos(phi)\n",
    "    y = r * jnp.sin(theta) * jnp.sin(phi)\n",
    "    z = r * jnp.cos(theta)\n",
    "    return jnp.stack([x, y, z], axis=-1)\n",
    "\n",
    "\n",
    "def xyz_to_spherical(pts):\n",
    "    x = pts[..., 0]\n",
    "    y = pts[..., 1]\n",
    "    z = pts[..., 2]\n",
    "\n",
    "    len_xy = jnp.hypot(x, y)\n",
    "    r = jnp.hypot(z, len_xy)\n",
    "    theta = jnp.arctan2(z, len_xy)\n",
    "    phi = jnp.arctan2(y, x)\n",
    "    return r, theta, phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate camera pose\n",
    "def get_camera_poses(\n",
    "    r, theta, phi, lookat=np.array([0, 0, 0.5]), up=np.array([0, 0, 1])\n",
    "):\n",
    "    # Camera position in world coord\n",
    "    cam_pts = spherical_to_xyz(r, theta, phi)\n",
    "    # Camera orientation\n",
    "    zaxis = lookat - cam_pts\n",
    "    zaxis /= np.linalg.norm(zaxis, axis=-1, keepdims=True)\n",
    "    xaxis = np.cross(zaxis, up)\n",
    "    xaxis /= np.linalg.norm(xaxis, axis=-1, keepdims=True)\n",
    "    yaxis = np.cross(zaxis, xaxis)\n",
    "\n",
    "    R = np.stack([xaxis, yaxis, zaxis], axis=-1)\n",
    "\n",
    "    # Transform matrix camera coord to world coord\n",
    "    T_wc = np.array([np.eye(4) for _ in range(cam_pts.shape[0])])\n",
    "    T_wc[..., :3, :3] = R\n",
    "    T_wc[..., :3, 3] = cam_pts\n",
    "    return T_wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = np.array()\n",
    "pose_num = 10\n",
    "r = np.full((pose_num), 3.5)\n",
    "theta = np.full((pose_num), np.deg2rad(80))\n",
    "phi = np.linspace(-np.pi, np.pi, pose_num, endpoint=False)\n",
    "cam_pts = spherical_to_xyz(r, theta, phi)\n",
    "\n",
    "cam_poses = get_camera_poses(r, theta, phi)\n",
    "\n",
    "# 3D Plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "# ax.axis('off')\n",
    "c = [\"c\", \"y\", \"k\", \"b\"]\n",
    "\n",
    "for i, pose in enumerate(cam_poses):\n",
    "    colors = [\"r\", \"g\", c[i % len(c)]]\n",
    "    drawAxis(pose, 0.6, colors)\n",
    "\n",
    "# Origin\n",
    "drawAxis()\n",
    "\n",
    "# ax.scatter(pts[...,0], pts[...,1], pts[...,2])\n",
    "\n",
    "# adjust scale\n",
    "scale = 7\n",
    "start = np.zeros((3, 3)) - scale / 2\n",
    "end = start + scale * np.eye(3)\n",
    "for s, e in zip(start, end):\n",
    "    ax.plot([s[0], e[0]], [s[1], e[1]], [s[2], e[2]], c=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, ch = imgs.shape[1:]\n",
    "cx = width / 2\n",
    "cy = height / 2\n",
    "print(f\"image height:{height}, width:{width}, f:{focal:.2f}, cx:{cx:.2f}, cy:{cy:.2f}\")\n",
    "\n",
    "# ray direction from cam\n",
    "x_grid = np.arange(width) - cx + 0.5\n",
    "y_grid = np.arange(height) - cy + 0.5\n",
    "x_grid, y_grid = np.meshgrid(x_grid, y_grid)\n",
    "ray_cam = np.stack([x_grid, y_grid, np.full_like(x_grid, focal)], axis=-1)\n",
    "\n",
    "# TODO divided by norm or z\n",
    "ray_cam /= np.linalg.norm(ray_cam, axis=-1, keepdims=True)\n",
    "ray_tmp = ray_cam[::10, ::10]  # For debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D Plot\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = Axes3D(fig)\n",
    "ax.set_xlabel(\"X\")\n",
    "ax.set_ylabel(\"Y\")\n",
    "ax.set_zlabel(\"Z\")\n",
    "# ax.axis('off')\n",
    "c = [\"c\", \"y\", \"k\", \"b\"]\n",
    "\n",
    "i = 0\n",
    "pose = poses[idx + i]\n",
    "colors = [\"r\", \"g\", c[i]]\n",
    "drawAxis(pose, 0.6, colors)\n",
    "\n",
    "# Camera optical center\n",
    "t = pose[:3, 3]\n",
    "# Cam coord to world coord rotation\n",
    "R = pose[:3, :3]\n",
    "\n",
    "ray_world = ray_tmp.dot(R.T) + t\n",
    "ray_world = ray_world.reshape(-1, 3)\n",
    "ax.scatter(ray_world[:, 0], ray_world[:, 1], ray_world[:, 2])\n",
    "\n",
    "# Origin\n",
    "drawAxis()\n",
    "\n",
    "# adjust scale\n",
    "scale = 7\n",
    "start = np.zeros((3, 3)) - scale / 2\n",
    "end = start + scale * np.eye(3)\n",
    "for s, e in zip(start, end):\n",
    "    ax.plot([s[0], e[0]], [s[1], e[1]], [s[2], e[2]], c=\"none\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeRF Model\n",
    "## Generate model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_variables(x, encode_num):\n",
    "    # No positional encoding\n",
    "    if encode_num == 0:\n",
    "        return x\n",
    "    # Positional encoding\n",
    "    frequency_bands = 2.0 ** jnp.linspace(0.0, encode_num - 1, encode_num)\n",
    "    encoded_grid = []\n",
    "    for freq in frequency_bands:\n",
    "        for func in [jnp.sin, jnp.cos]:\n",
    "            encoded_grid.append(func(jnp.pi * x * freq))\n",
    "    encoded_grid = jnp.concatenate(encoded_grid, axis=-1)\n",
    "    return encoded_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class NeRF(nn.Module):\n",
    "    layer_num: int\n",
    "    hidden_ch: int\n",
    "    encode_num: int\n",
    "    encode_num_view: int\n",
    "    skips: Tuple[int] = (4,)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # Encode input\n",
    "        xyz = x[:3]\n",
    "        view = x[3:]\n",
    "        xyz = encode_variables(xyz, self.encode_num)\n",
    "        view = encode_variables(view, self.encode_num_view)\n",
    "\n",
    "        # fc\n",
    "        x = xyz\n",
    "        for i in range(self.layer_num):\n",
    "            if i in self.skips:\n",
    "                x = jnp.concatenate([xyz, x])\n",
    "            x = nn.Dense(self.hidden_ch, name=f\"fc{i}\")(x)\n",
    "            x = jax.nn.relu(x)\n",
    "\n",
    "        # sigma\n",
    "        sigma_out = nn.Dense(1, name=f\"sigma_fc\")(x)\n",
    "        sigma = jax.nn.relu(sigma_out)\n",
    "\n",
    "        # rgb\n",
    "        bottleneck = nn.Dense(self.hidden_ch, name=\"rgb_fc0\")(x)\n",
    "        x = jnp.concatenate([view, bottleneck])\n",
    "        x = nn.Dense(self.hidden_ch // 2, name=f\"rgb_fc1\")(x)\n",
    "        x = jax.nn.relu(x)\n",
    "        x = nn.Dense(3, name=f\"rgb_fc2\")(x)\n",
    "        rgb = jax.nn.sigmoid(x)\n",
    "\n",
    "        return sigma, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    global cfg\n",
    "    return NeRF(\n",
    "        layer_num=cfg.layer_num,\n",
    "        hidden_ch=cfg.hidden_ch,\n",
    "        encode_num=cfg.encode_num,\n",
    "        encode_num_view=cfg.encode_num_view,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = random.normal(rng, (ch_dim + view_dim,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, k1, k2 = random.split(rng, num=3)\n",
    "params = {}\n",
    "params['coarse'] = model().init(k1, x)\n",
    "params['fine'] = model().init(k2, x)\n",
    "# out = model().apply(params, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# jax.tree_map(lambda x: x.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = testset[0]\n",
    "test_img = test_sample['img']\n",
    "test_pose = test_sample['pose']\n",
    "plt.imshow(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train images number:\", len(trainset))\n",
    "print(\"test images number:\", len(testset))\n",
    "trainloader = DataLoader(\n",
    "    trainset, batch_size=1, shuffle=True, collate_fn=collate_fn, num_workers=cfg.workers\n",
    ")\n",
    "testloader = DataLoader(\n",
    "    testset, batch_size=1, shuffle=False, collate_fn=collate_fn, num_workers=cfg.workers\n",
    ")\n",
    "\n",
    "# pixel ray direction in cam coord\n",
    "ray_dir_cam = jnp.asarray(ray_cam)\n",
    "ray_dist = jnp.linspace(cfg.near_clip, cfg.far_clip, cfg.ray_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(params, x):\n",
    "    sigma, rgb = model().apply(params, x)\n",
    "    return sigma, rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.ones((ch_dim + view_dim, ))\n",
    "sigma, rgb = prediction(params[\"coarse\"], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample points along one ray\n",
    "def _sample_points(ray_dir, t, ray_dist, camera_bnd=6.0):\n",
    "    # Sample points\n",
    "    pts = ray_dist[..., None] * ray_dir + t\n",
    "    \n",
    "    # Normalize for positional encoder\n",
    "    pts = pts/camera_bnd\n",
    "        \n",
    "    # With direction\n",
    "    use_theta_phi = False\n",
    "    if(use_theta_phi):\n",
    "        # With direction\n",
    "        r, theta, phi = xyz_to_spherical(ray_dir)\n",
    "\n",
    "        theta = theta/jnp.pi*2\n",
    "        phi = phi/jnp.pi\n",
    "\n",
    "        pts = jnp.concatenate(\n",
    "            [pts, jnp.full_like(pts[:, :1], theta), jnp.full_like(pts[:, :1], phi)], axis=-1\n",
    "        )\n",
    "    else:\n",
    "        pts = jnp.concatenate([pts, ray_dir[None].repeat(pts.shape[0], 0)], axis=-1)\n",
    "\n",
    "    # Distance\n",
    "    dists = ray_dist[1:] - ray_dist[:-1]\n",
    "    dists = jnp.pad(dists, (0, 1), 'constant', constant_values=(1e10))\n",
    "\n",
    "    return pts, dists\n",
    "\n",
    "sample_points = partial(_sample_points, camera_bnd=camera_bnd_max)\n",
    "\n",
    "def integrate_color(sigma, rgb, dists, bkg = jnp.array([1.0, 1.0, 1.0])):\n",
    "    transparency = jnp.exp(-sigma.squeeze() * dists)\n",
    "    alpha = 1 - transparency\n",
    "    # cumprod_exclusive\n",
    "    T = jnp.cumprod(transparency)[:-1]\n",
    "    T = jnp.pad(T, (1, 0), 'constant', constant_values=1)\n",
    "\n",
    "    # accumulate\n",
    "    weights = alpha[..., None]*T[..., None]\n",
    "    colors = rgb*weights\n",
    "    # Add background color\n",
    "    colors = colors.sum(axis=0) + bkg * (1.0 - weights.sum())\n",
    "\n",
    "    return colors, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction along a ray\n",
    "ray_prediction = vmap(prediction, in_axes=(None, 0))\n",
    "\n",
    "def render_ray(params, ray_dir, t, ray_dist):\n",
    "    pts, dists = sample_points(ray_dir, t, ray_dist)\n",
    "    sigma, rgb = ray_prediction(params, pts)\n",
    "    colors, weights = integrate_color(sigma, rgb, dists)\n",
    "    return colors, weights\n",
    "\n",
    "# Render multiple rays\n",
    "render_pixels = vmap(render_ray, in_axes=(None, 0, None, None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_pdf(weights, sample_num):\n",
    "    # Calculate pdf and cdf\n",
    "    weights = weights + 1e-5\n",
    "    pdf = weights/weights.sum()\n",
    "    cdf = jnp.cumsum(pdf)\n",
    "\n",
    "    # linspace(0, 1) without start or end points\n",
    "    start = 1/(sample_num+1)\n",
    "    end = 1 - start\n",
    "    uniform_prob = jnp.linspace(start, end, sample_num)\n",
    "\n",
    "    # TODO check out of range?\n",
    "    right_idx = jnp.searchsorted(cdf, uniform_prob, side='right')\n",
    "    left_idx = right_idx - 1\n",
    "\n",
    "    # Linear interpolation\n",
    "    right_factor = (uniform_prob - cdf[left_idx]) / (cdf[right_idx] - cdf[left_idx])\n",
    "    left_factor = 1.0 - right_factor\n",
    "    \n",
    "    return left_idx, left_factor\n",
    "\n",
    "\n",
    "def render_ray_fine(params, weights, ray_dir, fine_ray_sample, t, ray_dist):\n",
    "    # Importance sampling\n",
    "    l_idx, l_factor = sample_pdf(weights, fine_ray_sample)\n",
    "    fine_ray_dist = l_factor * ray_dist[l_idx] + (1 - l_factor) * ray_dist[l_idx + 1]\n",
    "    fine_ray_dist = jax.lax.stop_gradient(fine_ray_dist)\n",
    "    pts, dists = sample_points(ray_dir, t, fine_ray_dist)\n",
    "\n",
    "    # Color\n",
    "    sigma, rgb = ray_prediction(params, pts)\n",
    "    colors, weights = integrate_color(sigma, rgb, dists)\n",
    "    return colors, weights\n",
    "\n",
    "render_pixels_fine = vmap(render_ray_fine, in_axes=(None, 0, 0, None, None, None))\n",
    "\n",
    "# fine_colors, _ = render_pixels_fine(params, weights, batch_dirs, fine_ray_sample, t, ray_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(jit, static_argnums=2)\n",
    "def render_pixels_coarse_fine(params, batch_dirs, fine_ray_sample, t, ray_dist):\n",
    "    # Coarse sampling\n",
    "    coarse_colors, weights = render_pixels(params['coarse'], batch_dirs, t, ray_dist)\n",
    "    # Fine sampling\n",
    "    fine_colors, _ = render_pixels_fine(\n",
    "        params['fine'], weights, batch_dirs, fine_ray_sample, t, ray_dist\n",
    "    )\n",
    "    return fine_colors, coarse_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(params, batch_dirs, batch_pixels, t, ray_dist, fine_ray_sample):\n",
    "    # Coarse and fine sampling\n",
    "    fine_colors, coarse_colors = render_pixels_coarse_fine(\n",
    "        params, batch_dirs, fine_ray_sample, t, ray_dist\n",
    "    )\n",
    "    \n",
    "    # Calculate loss\n",
    "    coarse_loss = jnp.mean((batch_pixels - coarse_colors) ** 2)\n",
    "    fine_loss = jnp.mean((batch_pixels - fine_colors) ** 2)\n",
    "    total_loss = coarse_loss + fine_loss\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when you use @jit here, it causes out of gpu memory\n",
    "# I guess it's because @jit unroll for loop and try to execute once.\n",
    "# Just use @jit for core function, e.g., render_pixels_coarse_fine\n",
    "def render_image(params, pose, ray_dir_cam, fine_ray_sample):\n",
    "    # Camera optical center\n",
    "    t = pose[:3, 3]\n",
    "    # Cam coord to world coord rotation\n",
    "    R = pose[:3, :3]\n",
    "    # ray direction in world coord\n",
    "    ray_dir_world = ray_dir_cam.dot(R.T)\n",
    "    flatten_ray_dir_world = ray_dir_world.reshape(-1, 3)\n",
    "\n",
    "    coarse_colors = []\n",
    "    fine_colors = []\n",
    "    batch_ray_num = 4096\n",
    "    for pix_idx in range(0, len(flatten_ray_dir_world), batch_ray_num):\n",
    "        batch_dirs = flatten_ray_dir_world[pix_idx : pix_idx + batch_ray_num]\n",
    "        # Coarse and fine sampling\n",
    "        fine, coarse = render_pixels_coarse_fine(\n",
    "            params, batch_dirs, fine_ray_sample, t, ray_dist\n",
    "        )\n",
    "        coarse_colors.append(coarse)\n",
    "        fine_colors.append(fine)\n",
    "\n",
    "    # Convert colors into 2d image\n",
    "    coarse_img = jnp.concatenate(coarse_colors).reshape(ray_dir_cam.shape)\n",
    "    coarse_img = jnp.clip(coarse_img, a_min=0.0, a_max=1.0)\n",
    "    fine_img = jnp.concatenate(fine_colors).reshape(ray_dir_cam.shape)\n",
    "    fine_img = jnp.clip(fine_img, a_min=0.0, a_max=1.0)\n",
    "    return fine_img, coarse_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer_def = optim.Adam(learning_rate=cfg.lr)\n",
    "optimizer = optimizer_def.create(params)\n",
    "\n",
    "# LR scheduler\n",
    "def create_exponential_lr(initial_lr, decay_steps, decay_rate):\n",
    "    def learning_rate_fn(step):\n",
    "        lr = initial_lr*decay_rate**(step/decay_steps)\n",
    "        return lr\n",
    "    return learning_rate_fn\n",
    "\n",
    "# Final learning rate is cfg.lr*0.01\n",
    "learning_rate_fn = create_exponential_lr(cfg.lr, len(trainloader) * cfg.num_epochs / 2, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "\n",
    "def _optim_step(\n",
    "    optimizer, batch_dirs, batch_pixels, t, ray_dist, fine_ray_sample, learning_rate_fn\n",
    "):\n",
    "    loss_values, grads = value_and_grad(loss_fn)(\n",
    "        optimizer.target, batch_dirs, batch_pixels, t, ray_dist, fine_ray_sample\n",
    "    )\n",
    "    lr = learning_rate_fn(optimizer.state.step)\n",
    "    optimizer = optimizer.apply_gradient(grads, learning_rate=lr)\n",
    "    return loss_values, optimizer\n",
    "\n",
    "\n",
    "# Make fine_ray_sample and learning_rate_fun static args\n",
    "optim_step = jit(\n",
    "    partial(\n",
    "        _optim_step,\n",
    "        fine_ray_sample=cfg.fine_ray_sample,\n",
    "        learning_rate_fn=learning_rate_fn,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity\n",
    "\n",
    "\n",
    "def calculate_error_metrics(gt, pred):\n",
    "    gt = np.array(gt)\n",
    "    pred = np.array(pred)\n",
    "    mse = np.mean((gt - pred) ** 2)\n",
    "    psnr = -10 * np.log(mse) / np.log(10)\n",
    "    # compatible with tf.image.ssim\n",
    "    ssim = structural_similarity(\n",
    "        gt,\n",
    "        pred,\n",
    "        multichannel=True,\n",
    "        data_range=1.0,\n",
    "        win_size=11,\n",
    "        K1=0.01,\n",
    "        K2=0.03,\n",
    "        sigma=1.5,\n",
    "    )\n",
    "    return {\"psnr\": psnr, \"ssim\": ssim}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_ray_num = 4096\n",
    "interval_test_metrics = 100\n",
    "interval_save_model = 100\n",
    "interval_visualize_image = 10\n",
    "until_visualize_image = 5\n",
    "\n",
    "for epoch in range(cfg.num_epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    pbar = tqdm(trainloader, total=len(trainloader), desc=f\"[Train] Epoch:{epoch}\")\n",
    "    for batch in pbar:\n",
    "        img = batch[\"img\"][0]\n",
    "        pose = batch[\"pose\"][0]\n",
    "\n",
    "        # Camera optical center\n",
    "        t = pose[:3, 3]\n",
    "        # Cam coord to world coord rotation\n",
    "        R = pose[:3, :3]\n",
    "\n",
    "        # ray direction in world coord\n",
    "        ray_dir_world = ray_dir_cam.dot(R.T)\n",
    "        flatten_ray_dir_world = ray_dir_world.reshape(-1, 3)\n",
    "        flatten_image = img.reshape(-1, 3)\n",
    "\n",
    "        # Batch ray select index\n",
    "        HxW = flatten_ray_dir_world.shape[0]\n",
    "        batch_idx = np.array(sample_no_duplicates(range(HxW), batch_ray_num))\n",
    "\n",
    "        # Forward\n",
    "        batch_dirs = flatten_ray_dir_world[batch_idx]\n",
    "        batch_pixels = flatten_image[batch_idx]\n",
    "        loss_vals, optimizer = optim_step(\n",
    "            optimizer, batch_dirs, batch_pixels, t, ray_dist\n",
    "        )\n",
    "        loss_vals = float(loss_vals)  # jax array to float\n",
    "        total_loss += loss_vals\n",
    "        log_data = {\"loss\": loss_vals}\n",
    "\n",
    "        # Loss plot\n",
    "        pbar.set_postfix(log_data)\n",
    "#         wandb.log(log_data, commit=False)\n",
    "    print(f\"Epoch:{epoch}, total_loss:{total_loss}\")\n",
    "    # Plot lr for debug\n",
    "#     wandb.log({'lr':float(learning_rate_fn(optimizer.state.step))}, commit=False)\n",
    "\n",
    "    # Calculate test metrics\n",
    "    if not epoch % interval_test_metrics or epoch == (cfg.num_epochs - 1):\n",
    "        total_test_metrics = defaultdict(list)\n",
    "        pbar = tqdm(testloader, total=len(testloader), desc=f\"[Test] Epoch:{epoch}\")\n",
    "        for batch in pbar:\n",
    "            img = batch[\"img\"][0]\n",
    "            pose = batch[\"pose\"][0]\n",
    "            viz_img, _ = render_image(\n",
    "                optimizer.target, pose, ray_dir_cam, cfg.fine_ray_sample\n",
    "            )\n",
    "            test_metrics = calculate_error_metrics(img, viz_img)\n",
    "            for key, val in test_metrics.items():\n",
    "                total_test_metrics[key].append(val)\n",
    "\n",
    "        log_data = {}\n",
    "        for key, val in total_test_metrics.items():\n",
    "            log_data[\"test_ave_\" + key] = np.array(val).mean()\n",
    "        print(json.dumps(log_data, indent=1))\n",
    "#         wandb.log(log_data, commit=False)\n",
    "\n",
    "    # Visualize one test images\n",
    "    if not epoch % interval_visualize_image or epoch < until_visualize_image:\n",
    "        viz_img, coarse_img = render_image(\n",
    "            optimizer.target, testset[0][\"pose\"], ray_dir_cam, cfg.fine_ray_sample\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.set_title(f\"Epoch:{epoch}\")\n",
    "        ax.imshow(viz_img)\n",
    "        fig.tight_layout()\n",
    "#         wandb.log({\"result_imgs\": wandb.Image(fig)}, commit=False)\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.set_title(f\"coarse Epoch:{epoch}\")\n",
    "        ax.imshow(coarse_img)\n",
    "        fig.tight_layout()\n",
    "#         wandb.log({\"coarse_imgs(debug)\": wandb.Image(fig)}, commit=False)\n",
    "        plt.show()\n",
    "\n",
    "    # Commit log\n",
    "#     wandb.log({}, commit=True)\n",
    "\n",
    "    # Save model\n",
    "    if not epoch % interval_save_model or epoch == (cfg.num_epochs - 1):\n",
    "        fname = f\"checkpoints/nerf_jax_{epoch}.pkl\"\n",
    "        print(\"save model:\", fname)\n",
    "        with open(fname, mode=\"wb\") as f:\n",
    "            f.write(serialization.to_bytes(optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics using all testset\n",
    "total_test_metrics = defaultdict(list)\n",
    "alltestloader = DataLoader(all_testset, batch_size=1, shuffle=False, collate_fn=collate_fn, num_workers=cfg.workers)\n",
    "pbar = tqdm(alltestloader, total=len(alltestloader),desc=f\"[Final Test]\")\n",
    "for batch in pbar:\n",
    "    img = batch[\"img\"][0]\n",
    "    pose = batch[\"pose\"][0]\n",
    "    viz_img, _ = render_image(optimizer.target, pose, ray_dir_cam, cfg.fine_ray_sample)\n",
    "    test_metrics = calculate_error_metrics(img, viz_img)\n",
    "    pbar.set_postfix(test_metrics)\n",
    "    for key, val in test_metrics.items():\n",
    "        total_test_metrics[key].append(val)\n",
    "        \n",
    "log_data = {}\n",
    "for k, v in total_test_metrics.items():\n",
    "    key = 'final_ave_'+k\n",
    "    mean_val = np.array(v).mean()\n",
    "    log_data[key] = mean_val\n",
    "#     wandb.run.summary[key] = mean_val\n",
    "print(json.dumps(log_data, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_params = optimizer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load test\n",
    "# with open(fname, 'rb') as f:\n",
    "#     data = f.read()\n",
    "#     loaded_optimizer = serialization.from_bytes(optimizer, data)\n",
    "# load_params = loaded_optimizer.target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize test images\n",
    "img = test_img\n",
    "pose = test_pose\n",
    "viz_img, _ = render_image(load_params, pose, ray_dir_cam, cfg.fine_ray_sample)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.set_title(f\"Test visualization\")\n",
    "ax.imshow(viz_img)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_num = 100\n",
    "r = np.full((pose_num), 3.5)\n",
    "theta = np.full((pose_num), np.deg2rad(80))\n",
    "phi = np.linspace(-np.pi, np.pi, pose_num, endpoint=False)\n",
    "cam_poses = get_camera_poses(r, theta, phi).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_imgs = []\n",
    "for pose in tqdm(cam_poses):\n",
    "    viz_img, _ = render_image(load_params, pose, ray_dir_cam, cfg.fine_ray_sample)\n",
    "    viz_imgs.append(np.array(viz_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import IPython\n",
    "output_gifname = f\"results/nerf_jax_final_{epoch}.gif\"\n",
    "gif_imgs = [Image.fromarray((255*it).astype(np.uint8)) for it in viz_imgs]\n",
    "gif_imgs[0].save(\n",
    "    output_gifname,\n",
    "    save_all=True,\n",
    "    append_images=gif_imgs[1:],\n",
    "    optimize=False,\n",
    "    duration=50,\n",
    "    loop=0,\n",
    ")\n",
    "caption = f\"Final fine animation ({epoch})\"\n",
    "print(caption)\n",
    "IPython.display.Image(url=output_gifname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
